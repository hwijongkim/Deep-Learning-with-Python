{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.1 단어와 문자의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9개의 고유한 토큰을 찾았습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'cat',\n",
       " 3: 'sat',\n",
       " 4: 'on',\n",
       " 5: 'mat',\n",
       " 6: 'dog',\n",
       " 7: 'ate',\n",
       " 8: 'my',\n",
       " 9: 'homework'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 케라스를 사용한 단어 수준 one-hot encoding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples) # 단어 인덱스 구축\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples) # 문자열을 정수 인덱스 리스트로 변환\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n",
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot hashing : 어휘 사전에 고유 토큰 수가 너무 커서 모두 다루기 어려울 때 사용\n",
    "# 각 단어에 명시적으로 인덱스 할당하는 것이 아니라 단어를 해싱하여 고정된 크기의 벡터로 변환\n",
    "# 메모리 절약하고, 온라인 방식으로 데이터 인코딩할 수 있지만, 해시 충돌 발생 가능성 존재\n",
    "import numpy as np\n",
    "dimensionality = 1000 # 단어를 크기가 1000인 벡터로 저장. 1000개 이상의 단어가 존재할 경우 해싱 충돌이 늘어남\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality # 단어를 해싱하여 0~1000 사이의 랜덤한 정수 인덱스로 변환\n",
    "        results[i,j,index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 단어 임베딩 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원-핫 인코딩 : sparse / 고차원(어휘 사전에 있는 단어의 수와 동일 차원)\n",
    "- 단어 임베딩 : 저차원의 실수형 밀집 벡터\n",
    "    - 원-핫 인코딩(수동 인코딩)으로 얻은 단어 벡터와 달리 데이터로부터 학습\n",
    "    - 보통 256차원, 512차원을 사용하며 큰 어휘 사전의 경우 1024차원의 단어 임베딩 사용\n",
    "      (원-핫 인코딩의 경우 2만개 토큰의 어휘 사전일 경우 2만 차원 혹은 그 이상의 벡터가 만들어짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empedding 층을 사용하여 단어 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 층의 객체 생성하기\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64) # 파라미터: 토큰의 개수 / 임베딩 차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding층 입력: (samples, sequence_length)의 2D 정수 텐서\n",
    "- Embedding층 출력: (samples, sequence_length, embedding_dimensionality) 3D 실수형 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 영화 리뷰 감성 예측 문제로 실습\n",
    "# 가장 빈도 높은 1만개 단어 추출 후 20개 단어 이후는 버림\n",
    "# 1만개 단어에 대해 8차원의 임베딩을 학습하여 정수 시퀀스 입력(2D 정수 텐서)을\n",
    "# 임베딩 시퀀스(3D 실수형 텐서)로 변환\n",
    "# 그 후, 이 텐서를 2D로 펼쳐서 분류를 위한 Dense층 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000 #특성으로 사용할 단어 수\n",
    "maxlen = 20 # 사용할 텍스트 길이\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# 정수 리스트 로드\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hwibell\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_keras\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\hwibell\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_keras\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 129us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6814\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 68us/step - loss: 0.5657 - acc: 0.7427 - val_loss: 0.5467 - val_acc: 0.7206\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 73us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 70us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 71us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 73us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 75us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 72us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 70us/step - loss: 0.3022 - acc: 0.8766 - val_loss: 0.5213 - val_acc: 0.7490\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 71us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen)) #(samples, maxlen, 8) 출력\n",
    "\n",
    "model.add(Flatten()) # 위 출력을 2D 텐서(samples, maxlen*8)로 펼침\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train\n",
    "                    , epochs=10\n",
    "                    , batch_size=32\n",
    "                    , validation_split=0.2)\n",
    "\n",
    "# 약 75%의 검증 정확도 확보\n",
    "# but, 임베딩 시퀀스를 펼치고 하나의 Dense층을 훈련했으므로 입력 시퀀스에 있는 각 단어를 독립적으로 다룸\n",
    "# 즉, 단어 사이 관계나 문장구조를 고려하지 않음\n",
    "# 따라서 각 시퀀스 전체를 고려한 특성을 학습하도록 임베딩 층 위에 순환층이나 1D 합성곱 층을 추가하는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사전 훈련된 단어 임베딩 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미리 계산된 임베딩 공간에서 임베딩 백터를 로드\n",
    "- 위 공간은 언어 구조의 일반적인 측면을 잡아낼 수 있음\n",
    "- 충분한 데이터가 없어서 자신만의 좋은 특성을 학습하지 못하지만 꽤 일반적인 특성이 필요할 때 사용 like 위의 20개 단어 케이스\n",
    "---\n",
    "- **Word2vec 알고리즘**: 가장 성공적인 단어 임베딩 방법\n",
    "  - Word2vec의 차원은 <u>성별처럼 구체적인 의미가 있는 속성</u>을 잡아냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 케라스의 Embedding층을 위해 내려받을 수 있는 미리 계산된 단어 임베딩 데이터베이스  \n",
    "  1) Word2vec  \n",
    "  2) GloVe (by Stanford Univ., 2014)\n",
    "    - 단어의 동시 출현 통계를 기록한 행렬을 분해하는 기법 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.3 모든 내용을 적용하기: 원본 텍스트에서 단어 임베딩까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 IMDB 텍스트 내려받기\n",
    "# 훈련용 리뷰 하나를 문자열 하나로 만들어 훈련 데이터를 문자열의 리스트로 구성 후 긍/부정 labeling까지.\n",
    "import os\n",
    "imdb_dir = './datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clint Eastwood has definitely produced better movies than this, but this one does not embarrass him. Dirty Harry catches everyone's attention and unless one wants to watch romance, there is no reason why you won't like him. He is cool because he is dirty, is great because he kills without much thinking, is perfect because he gets the bullet right through your heart and a hero because he doesn't care.<br /><br />From what I have seen in movies in which Eastwood acts, the character of the lead role always captivates the audience. In White Hunter Black heart, he is the crazy director, in \"in the Line of Fire\" he is the \"Old 'un\" while here is the \"almost\" jobless with his job, that is to say he makes work for himself, doesn't care one damn about his superiors who practically send him out for a vacation.<br /><br />Based on a rape victim, this movie is promising for all the \"no non-sense\" movie watchers. The movie has nothing that goes away from he central plot. However, what makes it slightly inferior to the better movies of Eastwood is that though the character of the lead role is captivating the plot is not, as it is far too obvious from the beginning. It is not a movie that is going to make you sit at a place without moving. Also, there are too many people far dirtier than Dirty harry.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(texts[23000])\n",
    "print(labels[23000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88582개의 고유한 토큰을 찾았습니다.\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 벡터로 만들고 훈련세트와 검증세트 나누기\n",
    "# 사전 훈련된 단어 임베딩은 훈련 데이터가 부족할 때 특히 유용하므로 해당 상황 연출(훈련 데이터를 처음 200개 샘플로 제한)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100 # 100개 이후 단어는 버림\n",
    "training_samples = 200 # 훈련 샘플은 200개\n",
    "val_samples = 10000\n",
    "max_words = 10000 # 데이터셋에서 빈도 높은 1만개 단어만 사용\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 텐서의 크기:  (25000, 100)\n",
      "레이블 텐서의 크기:  (25000,)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('데이터 텐서의 크기: ', data.shape)\n",
    "print('레이블 텐서의 크기: ', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + val_samples]\n",
    "y_val = labels[training_samples: training_samples + val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape # wjdtn 100개씩 200 세트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe 단어 임베딩 내려받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_keras",
   "language": "python",
   "name": "tf_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
